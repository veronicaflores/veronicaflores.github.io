---
layout: post
title: Week 6
---

## Week 6 (August 1st - August 5th)

**August 2nd:** <br/>  

On this day, I came to Vicente with the tables I had previously curated from the Easy, Medium, and Hard tests I ran (as seen on the last blog post). The results showed that the CLIP model had very good recognition of the politicians being a person, as expected. Moreover, CLIP also had quite an easy time categorizing the politicians as such. Contrastingly, for the hard tests the model would give the text prompt "representative" for each type of politician. But, the second best matched text prompt was each respective type of politician. Here's a reference of the table that I posted on last post.

<img width="866" alt="Screen Shot 2022-12-12 at 2 12 18 PM" src="https://user-images.githubusercontent.com/52052151/207166235-e705fde7-a618-4db9-afa8-68ad34799b95.png">

Again, we looked at the various tables and saw what we could improve on. Moreover, I curated 3 different websites to graphically show the results of my work based on each Politician. Here are the links:
- [Senate Basic Tests](basic-tests-on-senate.html)  
- [House Basic Tests](basic-tests-on-house.html)  
- [Mayor Basic Tests](basic-tests-on-mayors.html) 

Based off the results I received we decided that we should add two more text prompts to the Easy Tests (text prompts after: person, dog, plant, giraffe, flag, tree, chair), three more text prompts to the Medium Tests (text prompts after: scientist, politician, athlete, teacher, receptionist, assistant, salesperson), and 1 more text prompt to the Hard Tests, while also changing a couple of words because the model was picking representative an abundant amount of times which skewed the results (text prompts after: the president, senator, mayor, US representative, the vicepresident, a governor, a commissioner). In addition we added a Harder 2.0 test that included the text prompts Senator, Mayor, and US Representative.

<img width="873" alt="Screen Shot 2022-12-12 at 2 19 52 PM" src="https://user-images.githubusercontent.com/52052151/207167464-12926a90-b1b3-4683-a74f-9afdd8d9773f.png">

**August 4th:** <br/> 

Today we decided that in order to analyze the potential societal biases we must split the basic tests (Easy, Medium, Hard) into tests by gender and by political affilation. We decided that we should not run the Easy tests since they where already very accurate. Additionally, Vicente and I decided that we should have two other tests besides the recogntion ones. Thus, we wanted to include a sentiment test where we tested clip on several positive and negative connotations related to the politicians. Here are some of the example prompts we wrote up:

<img width="409" alt="Screen Shot 2022-12-12 at 2 49 47 PM" src="https://user-images.githubusercontent.com/52052151/207171921-8e7a5bf6-7e43-4d3f-b941-34d10f5ea922.png">

In addition to the sentiment tests we decided on a third test that would test how the model knows about someone specifically based on their actual face? And for how many people?. Example prompts we drafted were:

[This is Bernie Sanders] [This is X] [This is Y] [This is Z] [This is W] [This is M] …
[This is the Senator from Vermont] [This is the Senator from X] [This is Senator from Y] … 

*Upcoming*
- Create updated tables for the Easy, Medium, Hard Tests 
- Complete tables for based on gender and political affiliation basic tests
- Create a Google Sheets with the following information of each Politician (Name, PictureURL/ID/Name, Political Party, Gender, Role [Mayor, Senator or US Representative], Location [State/City]
)
- Continue brainstorming tests
- New test: Sentiment Tests
