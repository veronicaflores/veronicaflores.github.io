---
layout: post
title: Week 2
---

## Week 2 (July 4th - July 8th)

**July 5th:**  

Professor Ordóñez Román and I met on July 5th after returning from a 3-day weekend (due to the 4th of July). On the 5th we discussed
the tests I ran with the **[CLIP Model](https://openai.com/blog/clip/)**. For further context the CLIP model jointly trains an image encoder and a text encoder to predict the 
correct pairings of a batch of (image, text) training examples. I ran the CLIP model on Google Collab by importing it from its original [Github Repository](https://github.com/openai/CLIP). 
I used the CLIP model to run some experiments using my own data, images, and text prompts. I would then see how the model responded to my original images. 
For the purpose of these experiments I used a picture of a water bottle, a bracelet, and a computer mouse. My results are as follows: 
