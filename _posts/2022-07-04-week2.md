---
layout: post
title: Week 2
---

## Week 2 (July 4th - July 8th)

**July 5th:** <br/>  
Professor Ordóñez Román and I met on July 5th after returning from a 3-day weekend (due to the 4th of July). On the 5th we discussed the tests I ran with the **[CLIP Model](https://openai.com/blog/clip/)**. CLIP models are trained via natural language supervision on a very large dataset and are capable of high zero-shot performance. For further context the CLIP model jointly trains an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) training examples. I ran the CLIP model on Google Collab by importing it from its original [Github Repository](https://github.com/openai/CLIP). I used the CLIP model to run some experiments using my own data, images, and text prompts. I would then see how the model responded to my original images. For the purpose of these experiments I used a picture of a water bottle, a bracelet, and a computer mouse.       

My results are as follows: 

</p><img width="871" alt="CLIP table" src="https://user-images.githubusercontent.com/52052151/178344706-21294443-6f84-4520-9cd4-c45c9c48f501.png">

How I displayed my data is through a table that displays each text prompt given to the image and what the probability indicated by the CLIP model stated. The closer the probability value is to 1 the more accurate the text prompt is according to the CLIP model. In the first round of tests I used easier text prompts while in the second round of tests I used text prompts that I believed may make it more confusing for the CLIP model. Based off these reults you can see how there was no probability except for the correct text prompt, in the first round, however, for the second round the probability values where more spread out. From these tests I gathered that the CLIP model can identify conventional everyday objects. 

**July 6th:** <br/>          
After performing these tests I used some of Professor Ordóñez Román's lectures and slides from his **[COMP 646: Deep Learning for Vision and Language](https://www.cs.rice.edu/~vo9/deep-vislang/)** class to get a better understanding of integral concepts in Visio and Language. His lectures and slides gave me a lot of insight on how vision and learning models function as well as the different features that each contain and what they mean. 

Along with the lectures I began some more tutorials on Python, including using pytorch and image classification. These tutorials are on Google Collab. Since I have not used Python with vision and learning, it is important that I am able to execute the tasks on the Collab notebooks correctly and comprehend them. Some things that I learned was the difference between Supervised and Unsupervised Learning. Moreover, some examples of supervised learning and how it works within machine learning (e.g. machine learning classfiers and regression models). 

**July 7th:** <br/>  
On the 7th, Professor Ordóñez Román, Eleanor (the other DREU intern working with Professor Ordóñez Román), and I got lunch at the POST Houston which is a food mall with a lot of variety. Since I haven't been around Houston much it was a cool thing to get to see and the terrace view was amazing, we all got sushi and then headed back to Rice University. 

<p float="left">
  <img src="https://user-images.githubusercontent.com/52052151/178346532-a6777253-60e5-4ec7-917e-3288d0a59ead.JPG" width="350" />
  <img src="https://user-images.githubusercontent.com/52052151/178348810-1618ced1-3abd-44fe-9bbf-accaa6687b74.JPG" width="350" /> 
</p>

**July 8th:** <br/> 
On friday I began running some experimental behavioral tests on the CLIP model (CLIP-ViT-B/32) using the Winoground challenge [dataset](https://huggingface.co/datasets/facebook/winoground) as discussed prior with my mentor. Before I began running these tests I read the *[Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality](https://arxiv.org/abs/2204.03162)* article
