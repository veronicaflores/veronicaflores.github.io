---
layout: post
title: Week 7
---

## Week 7 (August 8th - August 12th)

**August 9th:** <br/>  

Today Vicente and I looked over the completed tables I made for all the Basic Tests as well as my progress on the tests that where by gender and political affiliation. My results looked better in terms of the hard tests as the "US representative" text wasn't picked as much by CLIP. For the gender and political affiliation tests my mentor suggested that I also include the number of images for each sub-category. Here is an example of a semi completed table: 

<img width="871" alt="Screen Shot 2022-12-12 at 4 17 46 PM" src="https://user-images.githubusercontent.com/52052151/207193814-3d88b19e-0426-4605-824a-74ae0af25efb.png">

Furthermore, I created the google doc that would contain the demographics of all the politicians [here](https://docs.google.com/spreadsheets/d/1NHBhyhh3_KRQwJAFajz0STcwMBkNhojbgXi5E0LW_F4/edit?usp=sharing). Also, I began drafting the rest of the positive and negative connotations for the sentiment test:

<img width="406" alt="Screen Shot 2022-12-12 at 4 19 45 PM" src="https://user-images.githubusercontent.com/52052151/207194490-47795354-6ed7-46cd-be4b-6c58c428a99e.png">

Lastly, the last time we talked about the knowledge tests we talked about only names and states. However, this time we talked about creating 5-10 personal questions and testing the model with them. Moreover, we decided that the sum of these scores would be the the Personal Knowledge Score (PKS) which measures how much the model knows about a particular person. If the model can answer all five questions correctly then we assume the model knows a lot about the person. 

**August 12th:** <br/>  

I was incredibly excited about my mentor and I's meeting today as I had finished all the tests based on gender and political affiliation as well as the aggregated scores. Here are some of the results:

<img width="885" alt="Screen Shot 2022-12-12 at 4 30 10 PM" src="https://user-images.githubusercontent.com/52052151/207197010-0c35172c-823a-4cd4-a065-2b7d635b63e4.png">

<img width="872" alt="Screen Shot 2022-12-12 at 4 30 38 PM" src="https://user-images.githubusercontent.com/52052151/207197052-229eaccd-00e3-433c-8858-4916e5b6c81e.png">

Aside from the completed tables I ran the Sentiment tests on gender and political affiliation assigning a percntage to each category. The scores were calculated based on the average amount of positive attributes associated with the candidate, each candidate received a score out of 10 (a 1 was added when a positive sentence was associated and 0 when a negative score was associated), then the table was broken up into several categories. For example: The average score give to a women in the senate was 42.50% (or 4.25/10). Which means the women where matched to about 4.25 out of the 10 positive statements given. An example text prompt was “This is a photo showing an honest person” "This a photo showing a dishonest person".

<img width="873" alt="Screen Shot 2022-12-12 at 4 32 50 PM" src="https://user-images.githubusercontent.com/52052151/207197266-4cc6c409-efc0-4d0a-a7bb-600509f3603f.png">

Lastly, I created the data report.

<img width="264" alt="Screen Shot 2022-12-12 at 4 37 01 PM" src="https://user-images.githubusercontent.com/52052151/207197729-76168dcd-4e7c-42e0-bbe1-c8e171c1e7c5.png">

*Upcoming* 
- For the hard test maybe let’s change US representative for something less generic – “US House Representative” — or “House representative” 
- Tables for each question in the Sentiment Testing
- Webpage for Individual PKS Score (ranked from greatest to least)
-  Read [Debiasing word embeddings using PCA](https://arxiv.org/abs/1607.06520)
    - Principal Component Analysis
